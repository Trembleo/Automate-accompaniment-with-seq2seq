{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "4PRCNBP6dM_a",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Project requirements& prerequisites\n",
    "\n",
    "### python3.0+\n",
    "### keras using tensorflow backends\n",
    "### glob, pandas, numpy for data processing\n",
    "### music21 for computer musicology\n",
    "\n",
    "### Using Google Colab as compiler(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "id": "d15R5vKeo9ct",
    "new_sheet": false,
    "outputId": "20b16893-4718-4747-bc5d-d47ebd664c4c",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: music21 in /usr/local/lib/python3.6/dist-packages (5.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install music21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "3kz76WapfcDR",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Parsing midi files from Google Drive and convert all the information needed properly (mostly into list) and save  them as .csv files for further uses\n",
    "\n",
    "Here intend to make the model easier to train, we choose to transpose all the midi files (and all the information within them like notes and chords) to the same key (Cmaj or Amin).\n",
    "\n",
    "We treat the midi stream in any single measure as a \"sentence\". And we put all the \"sentences\" together and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4848
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "id": "OAte0Qa_FDNY",
    "new_sheet": false,
    "outputId": "a6257984-90ee-4f85-8305-7c571efab105",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /gdrive\n",
      "/gdrive/My Drive/MIDI files/popM/twins-下一站天后.mid\n",
      "Bmajor\n",
      "1\n",
      "63\n",
      "63\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-你好吗.mid\n",
      "Gmajor\n",
      "5\n",
      "111\n",
      "111\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-兰亭序.mid\n",
      "Dmajor\n",
      "-2\n",
      "190\n",
      "190\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-听见下雨的声音.mid\n",
      "Emajor\n",
      "-4\n",
      "276\n",
      "276\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-夜曲.mid\n",
      "Eminor\n",
      "5\n",
      "412\n",
      "412\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-天涯过客.mid\n",
      "Dmajor\n",
      "-2\n",
      "458\n",
      "458\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-安静.mid\n",
      "B-major\n",
      "2\n",
      "557\n",
      "557\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-彩虹.mid\n",
      "Cmajor\n",
      "0\n",
      "639\n",
      "639\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-明明就.mid\n",
      "Amajor\n",
      "3\n",
      "664\n",
      "664\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-烟花易冷.mid\n",
      "Aminor\n",
      "0\n",
      "703\n",
      "703\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-爱你没差.mid\n",
      "Fmajor\n",
      "-5\n",
      "787\n",
      "787\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-珊瑚海.mid\n",
      "Amajor\n",
      "3\n",
      "826\n",
      "826\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-甜甜的.mid\n",
      "Emajor\n",
      "-4\n",
      "861\n",
      "861\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-菊花台.mid\n",
      "Gmajor\n",
      "5\n",
      "932\n",
      "932\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-蒲公英的约定.mid\n",
      "Cmajor\n",
      "0\n",
      "1001\n",
      "1001\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-蜗牛.mid\n",
      "Gmajor\n",
      "5\n",
      "1057\n",
      "1057\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-说了再见.mid\n",
      "Dmajor\n",
      "-2\n",
      "1092\n",
      "1092\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-说好的幸福呢.mid\n",
      "Cmajor\n",
      "0\n",
      "1130\n",
      "1130\n",
      "/gdrive/My Drive/MIDI files/popM/周杰伦-青花瓷.mid\n",
      "Amajor\n",
      "3\n",
      "1233\n",
      "1233\n",
      "/gdrive/My Drive/MIDI files/popM/张国荣-追.mid\n",
      "Dmajor\n",
      "-2\n",
      "1271\n",
      "1271\n",
      "/gdrive/My Drive/MIDI files/popM/张惠妹-聽海.mid\n",
      "B-major\n",
      "2\n",
      "1344\n",
      "1344\n",
      "/gdrive/My Drive/MIDI files/popM/李克勤-回首.mid\n",
      "Aminor\n",
      "0\n",
      "1415\n",
      "1415\n",
      "/gdrive/My Drive/MIDI files/popM/李克勤-深深深.mid\n",
      "Dminor\n",
      "-5\n",
      "1478\n",
      "1478\n",
      "/gdrive/My Drive/MIDI files/popM/林俊杰-一千年以后.mid\n",
      "Amajor\n",
      "3\n",
      "1533\n",
      "1533\n",
      "/gdrive/My Drive/MIDI files/popM/林俊杰-她说.mid\n",
      "Cmajor\n",
      "0\n",
      "1585\n",
      "1585\n",
      "/gdrive/My Drive/MIDI files/popM/林俊杰-江南.mid\n",
      "Gminor\n",
      "2\n",
      "1636\n",
      "1636\n",
      "/gdrive/My Drive/MIDI files/popM/林俊杰-简简单单.mid\n",
      "Gmajor\n",
      "5\n",
      "1658\n",
      "1658\n",
      "/gdrive/My Drive/MIDI files/popM/林俊杰-背对背拥抱.mid\n",
      "E-major\n",
      "-3\n",
      "1721\n",
      "1721\n",
      "/gdrive/My Drive/MIDI files/popM/林俊杰-醉赤壁.mid\n",
      "A-major\n",
      "4\n",
      "1814\n",
      "1814\n",
      "/gdrive/My Drive/MIDI files/popM/范晓萱-rain.mid\n",
      "Cmajor\n",
      "0\n",
      "1920\n",
      "1920\n",
      "/gdrive/My Drive/MIDI files/popM/范晓萱-眼淚.mid\n",
      "Cmajor\n",
      "0\n",
      "2007\n",
      "2007\n",
      "/gdrive/My Drive/MIDI files/popM/许嵩-断桥残雪.mid\n",
      "Aminor\n",
      "0\n",
      "2073\n",
      "2073\n",
      "/gdrive/My Drive/MIDI files/popM/许美静-倾城.mid\n",
      "Gmajor\n",
      "5\n",
      "2136\n",
      "2136\n",
      "/gdrive/My Drive/MIDI files/popM/谭咏麟-无言感激.mid\n",
      "Gmajor\n",
      "5\n",
      "2214\n",
      "2214\n",
      "/gdrive/My Drive/MIDI files/popM/谭咏麟-无边的思忆.mid\n",
      "Bminor\n",
      "-2\n",
      "2299\n",
      "2299\n",
      "/gdrive/My Drive/MIDI files/popM/谭咏麟-爱念.mid\n",
      "Eminor\n",
      "5\n",
      "2369\n",
      "2369\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-人来人往.mid\n",
      "Gmajor\n",
      "5\n",
      "2439\n",
      "2439\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-从何说起.mid\n",
      "Fmajor\n",
      "-5\n",
      "2472\n",
      "2472\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-你给我听好.mid\n",
      "Gmajor\n",
      "5\n",
      "2535\n",
      "2535\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-十年.mid\n",
      "Fminor\n",
      "4\n",
      "2583\n",
      "2583\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-单车.mid\n",
      "Gmajor\n",
      "5\n",
      "2636\n",
      "2636\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-夕阳无限好.mid\n",
      "B-major\n",
      "2\n",
      "2717\n",
      "2717\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-好久不见.mid\n",
      "Cmajor\n",
      "0\n",
      "2788\n",
      "2788\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-如果我是陈奕迅.mid\n",
      "Cminor\n",
      "-3\n",
      "2892\n",
      "2892\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-孤独患者.mid\n",
      "Gmajor\n",
      "5\n",
      "2938\n",
      "2938\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-完.mid\n",
      "A-major\n",
      "4\n",
      "2995\n",
      "2995\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-富士山下.mid\n",
      "Dminor\n",
      "-5\n",
      "3103\n",
      "3103\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-岁月如歌.mid\n",
      "E-major\n",
      "-3\n",
      "3162\n",
      "3162\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-淘汰.mid\n",
      "B-major\n",
      "2\n",
      "3230\n",
      "3230\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-爱情转移.mid\n",
      "Dminor\n",
      "-5\n",
      "3338\n",
      "3338\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-红玫瑰.mid\n",
      "Aminor\n",
      "0\n",
      "3460\n",
      "3460\n",
      "/gdrive/My Drive/MIDI files/popM/陈奕迅-陪你走过漫长岁月.mid\n",
      "A-major\n",
      "4\n",
      "3500\n",
      "3500\n",
      "                              melody                         accompany\n",
      "0       [E5, G5, D6, C6, G5, E5, G5]  [C3, G3, C4, G3, F2, C3, F3, A3]\n",
      "1    [D5, G5, D6, C6, G5, 11.4, 0.4]  [G2, D3, G3, D3, A2, E3, A3, G3]\n",
      "2  [B5, C6, B5, A5, G5, 2.5, E5, D5]             [F2, C3, F3, A3, 2.7]\n",
      "3              [C5, G4, C5, D5, 4.7]          [C3, G3, C4, C3, G3, C4]\n",
      "4       [E5, E5, E5, G5, D5, D5, G5]          [C3, G3, C4, B2, G3, B3]\n",
      "                                prev                               next\n",
      "0       [E5, G5, D6, C6, G5, E5, G5]    [D5, G5, D6, C6, G5, 11.4, 0.4]\n",
      "1    [D5, G5, D6, C6, G5, 11.4, 0.4]  [B5, C6, B5, A5, G5, 2.5, E5, D5]\n",
      "2  [B5, C6, B5, A5, G5, 2.5, E5, D5]              [C5, G4, C5, D5, 4.7]\n",
      "3              [C5, G4, C5, D5, 4.7]       [E5, E5, E5, G5, D5, D5, G5]\n",
      "4       [E5, E5, E5, G5, D5, D5, G5]               [C5, C5, C5, E5, A4]\n",
      "   start  length\n",
      "0      0      63\n",
      "1     63      48\n",
      "2    111      79\n",
      "3    190      86\n",
      "4    276     136\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "from google.colab import drive\n",
    "drive.mount('/gdrive')\n",
    "from music21 import midi, interval, pitch, note, chord\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "midifile_path = glob.glob(\"/gdrive/My Drive/MIDI files/popM/*.mid\")\n",
    "midifile_path.sort(key= str.lower)\n",
    "\n",
    "melodyLists = []\n",
    "accoLists = []\n",
    "\n",
    "melody_Beat_List=[]\n",
    "acco_Beat_List=[]\n",
    "\n",
    "barIndex = []\n",
    "def setKeys(midi):\n",
    "    print(str(midi.analyze('key').tonic) + midi.analyze('key').mode)\n",
    "    keyTonic = midi.analyze('key').tonic\n",
    "    mode = midi.analyze('key').mode\n",
    "    #The pitch of the tone of the original key\n",
    "    originPitch = pitch.Pitch(keyTonic)\n",
    "    #set target key\n",
    "    if mode == \"major\":\n",
    "        targetPitch = pitch.Pitch('C')\n",
    "        pitchInterval = interval.notesToChromatic(originPitch,targetPitch).semitones\n",
    "        if pitchInterval > 6:\n",
    "            pitchInterval = pitchInterval-12\n",
    "        elif pitchInterval < -6:\n",
    "            pitchInterval = pitchInterval+12\n",
    "        print(pitchInterval)\n",
    "        if keyTonic != \"C\":\n",
    "            midi.transpose(pitchInterval ,inPlace=True)\n",
    "    elif mode == \"minor\":\n",
    "        targetPitch = pitch.Pitch('A')\n",
    "        pitchInterval = interval.notesToChromatic(originPitch,targetPitch).semitones\n",
    "        if pitchInterval > 6:\n",
    "            pitchInterval = pitchInterval-12\n",
    "        elif pitchInterval < -6:\n",
    "            pitchInterval = pitchInterval+12\n",
    "        print(pitchInterval)\n",
    "        if keyTonic != \"A\":\n",
    "            midi.transpose(pitchInterval ,inPlace=True)\n",
    "    return midi\n",
    "\n",
    "\n",
    "def parseMelody(midi):\n",
    "    midi = midi.makeMeasures()\n",
    "    measures = midi.recurse(classFilter=('Measure'), restoreActiveSites=False).elements\n",
    "    barIndex.append((len(melodyLists),len(measures)))\n",
    "    for measure in measures:\n",
    "        elements = []\n",
    "        note_or_chord = measure.recurse(classFilter=('Note','Chord'), restoreActiveSites=False).elements\n",
    "        for ele in note_or_chord:\n",
    "            if isinstance(ele, note.Note):\n",
    "                elements.append(str(ele.pitch))\n",
    "            elif isinstance(ele, chord.Chord):\n",
    "                ele = '.'.join(str(c) for c in ele.normalOrder)\n",
    "                elements.append(ele)\n",
    "            else:\n",
    "                elements.append(str(ele.name))\n",
    "        melodyLists.append(elements)\n",
    "\n",
    "def parseAcco(midi):\n",
    "    midi = midi.makeMeasures()\n",
    "    measures = midi.recurse(classFilter=('Measure'), restoreActiveSites=False).elements\n",
    "    for measure in measures:\n",
    "        elements = []\n",
    "        note_or_chord = measure.recurse(classFilter=('Note','Chord'), restoreActiveSites=False).elements\n",
    "        for ele in note_or_chord:\n",
    "            if isinstance(ele, note.Note):\n",
    "                elements.append(str(ele.pitch))\n",
    "            elif isinstance(ele, chord.Chord):\n",
    "                ele = '.'.join(str(c) for c in ele.normalOrder)\n",
    "                elements.append(ele)\n",
    "            else:\n",
    "                elements.append(str(ele.name))\n",
    "        accoLists.append(elements)\n",
    "\n",
    "def parseBeat(midi, list):\n",
    "    midi = midi.makeMeasures(inPlace=False)\n",
    "    measures = midi.recurse(classFilter=('Measure'), restoreActiveSites=False).elements\n",
    "    for measure in measures:\n",
    "        note_or_chord = measure.recurse(classFilter=('Note','Chord'), restoreActiveSites=False).elements\n",
    "        n = [float(t.offset) for t in note_or_chord]\n",
    "        list.append(n)\n",
    "    return list\n",
    "\n",
    "for file in midifile_path:\n",
    "    print(file)\n",
    "    midifile = midi.translate.midiFilePathToStream(file)\n",
    "    midifile = setKeys(midifile)\n",
    "    melody = midifile.parts[0]\n",
    "    parseMelody(melody)\n",
    "    melody_Beat_List = parseBeat(melody,melody_Beat_List)\n",
    "    accompany = midifile.parts[1]\n",
    "    parseAcco(accompany)\n",
    "    acco_Beat_List = parseBeat(accompany,acco_Beat_List)\n",
    "\n",
    "melodyLists = np.reshape(melodyLists, (len(melodyLists),1))\n",
    "accoLists = np.reshape(accoLists, (len(accoLists),1))\n",
    "\n",
    "target_MA = np.concatenate((melodyLists,accoLists),axis=1)\n",
    "target_MM = np.concatenate((melodyLists[:-1],melodyLists[1:]),axis=1)\n",
    "\n",
    "df_MA = pd.DataFrame(target_MA,columns=['melody','accompany'])\n",
    "print(df_MA.head())\n",
    "df_MA.to_csv('Melody_Accompany.csv')\n",
    "\n",
    "df_MM = pd.DataFrame(target_MM,columns=['prev','next'])\n",
    "print(df_MM.head())\n",
    "df_MM.to_csv('Melody_Melody.csv')\n",
    "\n",
    "df_beat_M = pd.DataFrame(melody_Beat_List)\n",
    "df_beat_A = pd.DataFrame(acco_Beat_List)\n",
    "\n",
    "df_beat_M.to_csv('beats_of_Melody.csv')\n",
    "df_beat_A.to_csv('beats_of_Accompany.csv')\n",
    "\n",
    "df_BI = pd.DataFrame(barIndex, columns=['start','length'])\n",
    "print(df_BI.head())\n",
    "df_BI.to_csv('bar_index.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "EmYeJzoyg6xO",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### The training model is a \"word-level\" seq2seq model derived from the keras NMT model \n",
    "https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py\n",
    "\n",
    "Now let's parse the .csv file we just saved. 'Melody' and 'Accompany' are two different lists of the same length, representing a collection of input and output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5664
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "id": "VsMlsvWZq4KG",
    "new_sheet": false,
    "outputId": "0a55ade1-c734-4757-b4d9-4e95eb09a5a3",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              melody  \\\n",
      "0         ['E5', 'G5', 'D6', 'C6', 'G5', 'E5', 'G5']   \n",
      "1      ['D5', 'G5', 'D6', 'C6', 'G5', '11.4', '0.4']   \n",
      "2  ['B5', 'C6', 'B5', 'A5', 'G5', '2.5', 'E5', 'D5']   \n",
      "3                    ['C5', 'G4', 'C5', 'D5', '4.7']   \n",
      "4         ['E5', 'E5', 'E5', 'G5', 'D5', 'D5', 'G5']   \n",
      "\n",
      "                                          accompany  \n",
      "0  ['C3', 'G3', 'C4', 'G3', 'F2', 'C3', 'F3', 'A3']  \n",
      "1  ['G2', 'D3', 'G3', 'D3', 'A2', 'E3', 'A3', 'G3']  \n",
      "2                   ['F2', 'C3', 'F3', 'A3', '2.7']  \n",
      "3              ['C3', 'G3', 'C4', 'C3', 'G3', 'C4']  \n",
      "4              ['C3', 'G3', 'C4', 'B2', 'G3', 'B3']  \n",
      "Number of samples: 3500\n",
      "Number of unique input tokens: 188\n",
      "Number of unique output tokens: 195\n",
      "Max sequence length for inputs: 30\n",
      "Max sequence length for outputs: 20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 2800 samples, validate on 700 samples\n",
      "Epoch 1/150\n",
      "2800/2800 [==============================] - 12s 4ms/step - loss: 1.2931 - val_loss: 1.2462\n",
      "Epoch 2/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 1.2137 - val_loss: 1.2245\n",
      "Epoch 3/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 1.1270 - val_loss: 1.1711\n",
      "Epoch 4/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 1.0363 - val_loss: 1.1224\n",
      "Epoch 5/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.9744 - val_loss: 1.0592\n",
      "Epoch 6/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.9294 - val_loss: 1.0353\n",
      "Epoch 7/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.8895 - val_loss: 1.0636\n",
      "Epoch 8/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.8552 - val_loss: 1.0385\n",
      "Epoch 9/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.8226 - val_loss: 1.0215\n",
      "Epoch 10/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.7929 - val_loss: 1.0265\n",
      "Epoch 11/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.7653 - val_loss: 1.0225\n",
      "Epoch 12/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.7377 - val_loss: 1.0217\n",
      "Epoch 13/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.7121 - val_loss: 1.0285\n",
      "Epoch 14/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6886 - val_loss: 1.0278\n",
      "Epoch 15/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6647 - val_loss: 1.0239\n",
      "Epoch 16/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6414 - val_loss: 1.0419\n",
      "Epoch 17/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6212 - val_loss: 1.0315\n",
      "Epoch 18/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.6023 - val_loss: 1.0392\n",
      "Epoch 19/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.5826 - val_loss: 1.0569\n",
      "Epoch 20/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.5643 - val_loss: 1.0592\n",
      "Epoch 21/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.5477 - val_loss: 1.0630\n",
      "Epoch 22/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.5284 - val_loss: 1.0660\n",
      "Epoch 23/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.5161 - val_loss: 1.0795\n",
      "Epoch 24/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.4995 - val_loss: 1.0896\n",
      "Epoch 25/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.4881 - val_loss: 1.0724\n",
      "Epoch 26/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.4704 - val_loss: 1.1173\n",
      "Epoch 27/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.4575 - val_loss: 1.1018\n",
      "Epoch 28/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.4432 - val_loss: 1.1391\n",
      "Epoch 29/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.4314 - val_loss: 1.1366\n",
      "Epoch 30/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.4181 - val_loss: 1.1431\n",
      "Epoch 31/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.4064 - val_loss: 1.1585\n",
      "Epoch 32/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3949 - val_loss: 1.1612\n",
      "Epoch 33/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3806 - val_loss: 1.1849\n",
      "Epoch 34/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3688 - val_loss: 1.1842\n",
      "Epoch 35/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3583 - val_loss: 1.1921\n",
      "Epoch 36/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3461 - val_loss: 1.1986\n",
      "Epoch 37/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3356 - val_loss: 1.2278\n",
      "Epoch 38/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3237 - val_loss: 1.2337\n",
      "Epoch 39/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3167 - val_loss: 1.2255\n",
      "Epoch 40/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.3048 - val_loss: 1.2438\n",
      "Epoch 41/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2954 - val_loss: 1.2578\n",
      "Epoch 42/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2859 - val_loss: 1.2889\n",
      "Epoch 43/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2752 - val_loss: 1.2819\n",
      "Epoch 44/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2677 - val_loss: 1.2992\n",
      "Epoch 45/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2585 - val_loss: 1.3039\n",
      "Epoch 46/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2510 - val_loss: 1.3188\n",
      "Epoch 47/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2418 - val_loss: 1.3541\n",
      "Epoch 48/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2333 - val_loss: 1.3767\n",
      "Epoch 49/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2254 - val_loss: 1.3604\n",
      "Epoch 50/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2168 - val_loss: 1.3679\n",
      "Epoch 51/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2112 - val_loss: 1.3696\n",
      "Epoch 52/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.2039 - val_loss: 1.3981\n",
      "Epoch 53/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1970 - val_loss: 1.3893\n",
      "Epoch 54/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1893 - val_loss: 1.4118\n",
      "Epoch 55/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1827 - val_loss: 1.4220\n",
      "Epoch 56/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1784 - val_loss: 1.4632\n",
      "Epoch 57/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1719 - val_loss: 1.4687\n",
      "Epoch 58/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1652 - val_loss: 1.4785\n",
      "Epoch 59/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1582 - val_loss: 1.4975\n",
      "Epoch 60/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1541 - val_loss: 1.4860\n",
      "Epoch 61/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1505 - val_loss: 1.5065\n",
      "Epoch 62/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1432 - val_loss: 1.5087\n",
      "Epoch 63/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1395 - val_loss: 1.5245\n",
      "Epoch 64/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1336 - val_loss: 1.5210\n",
      "Epoch 65/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1419 - val_loss: 1.5656\n",
      "Epoch 66/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1281 - val_loss: 1.5600\n",
      "Epoch 67/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1228 - val_loss: 1.5675\n",
      "Epoch 68/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1193 - val_loss: 1.5483\n",
      "Epoch 69/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1155 - val_loss: 1.5602\n",
      "Epoch 70/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1109 - val_loss: 1.5920\n",
      "Epoch 71/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1100 - val_loss: 1.6067\n",
      "Epoch 72/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1051 - val_loss: 1.6194\n",
      "Epoch 73/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.1009 - val_loss: 1.6072\n",
      "Epoch 74/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0994 - val_loss: 1.6233\n",
      "Epoch 75/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0966 - val_loss: 1.6297\n",
      "Epoch 76/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0931 - val_loss: 1.6375\n",
      "Epoch 77/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0904 - val_loss: 1.6303\n",
      "Epoch 78/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0882 - val_loss: 1.6741\n",
      "Epoch 79/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0862 - val_loss: 1.6577\n",
      "Epoch 80/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0835 - val_loss: 1.6701\n",
      "Epoch 81/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0814 - val_loss: 1.6875\n",
      "Epoch 82/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0785 - val_loss: 1.6894\n",
      "Epoch 83/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0766 - val_loss: 1.6906\n",
      "Epoch 84/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0755 - val_loss: 1.6966\n",
      "Epoch 85/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0732 - val_loss: 1.6997\n",
      "Epoch 86/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0707 - val_loss: 1.6837\n",
      "Epoch 87/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0700 - val_loss: 1.6828\n",
      "Epoch 88/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0677 - val_loss: 1.7109\n",
      "Epoch 89/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0670 - val_loss: 1.7260\n",
      "Epoch 90/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0648 - val_loss: 1.7209\n",
      "Epoch 91/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0622 - val_loss: 1.7474\n",
      "Epoch 92/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0608 - val_loss: 1.7536\n",
      "Epoch 93/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0612 - val_loss: 1.7764\n",
      "Epoch 94/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0585 - val_loss: 1.7534\n",
      "Epoch 95/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0584 - val_loss: 1.7850\n",
      "Epoch 96/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0575 - val_loss: 1.7605\n",
      "Epoch 97/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0559 - val_loss: 1.8035\n",
      "Epoch 98/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0549 - val_loss: 1.7937\n",
      "Epoch 99/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0546 - val_loss: 1.7906\n",
      "Epoch 100/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0532 - val_loss: 1.8006\n",
      "Epoch 101/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0528 - val_loss: 1.7879\n",
      "Epoch 102/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0531 - val_loss: 1.7741\n",
      "Epoch 103/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0560 - val_loss: 1.7799\n",
      "Epoch 104/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0515 - val_loss: 1.7888\n",
      "Epoch 105/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0494 - val_loss: 1.8130\n",
      "Epoch 106/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0486 - val_loss: 1.8144\n",
      "Epoch 107/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0496 - val_loss: 1.8052\n",
      "Epoch 108/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0489 - val_loss: 1.8202\n",
      "Epoch 109/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0462 - val_loss: 1.8010\n",
      "Epoch 110/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0471 - val_loss: 1.8341\n",
      "Epoch 111/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0497 - val_loss: 1.8228\n",
      "Epoch 112/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0452 - val_loss: 1.8194\n",
      "Epoch 113/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0449 - val_loss: 1.8134\n",
      "Epoch 114/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0442 - val_loss: 1.8367\n",
      "Epoch 115/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0462 - val_loss: 1.8467\n",
      "Epoch 116/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0429 - val_loss: 1.8577\n",
      "Epoch 117/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0432 - val_loss: 1.8349\n",
      "Epoch 118/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0422 - val_loss: 1.8296\n",
      "Epoch 119/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0430 - val_loss: 1.8407\n",
      "Epoch 120/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0426 - val_loss: 1.8654\n",
      "Epoch 121/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0422 - val_loss: 1.8608\n",
      "Epoch 122/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0420 - val_loss: 1.8674\n",
      "Epoch 123/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0424 - val_loss: 1.8674\n",
      "Epoch 124/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0425 - val_loss: 1.8510\n",
      "Epoch 125/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0411 - val_loss: 1.8672\n",
      "Epoch 126/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0400 - val_loss: 1.8694\n",
      "Epoch 127/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0402 - val_loss: 1.8609\n",
      "Epoch 128/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0402 - val_loss: 1.8755\n",
      "Epoch 129/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0399 - val_loss: 1.8950\n",
      "Epoch 130/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0389 - val_loss: 1.8677\n",
      "Epoch 131/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0386 - val_loss: 1.8793\n",
      "Epoch 132/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0393 - val_loss: 1.8791\n",
      "Epoch 133/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0395 - val_loss: 1.8717\n",
      "Epoch 134/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0392 - val_loss: 1.9327\n",
      "Epoch 135/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0386 - val_loss: 1.8962\n",
      "Epoch 136/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0381 - val_loss: 1.8865\n",
      "Epoch 137/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0376 - val_loss: 1.9100\n",
      "Epoch 138/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0384 - val_loss: 1.8973\n",
      "Epoch 139/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0378 - val_loss: 1.9194\n",
      "Epoch 140/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0363 - val_loss: 1.8997\n",
      "Epoch 141/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0379 - val_loss: 1.9011\n",
      "Epoch 142/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0365 - val_loss: 1.8876\n",
      "Epoch 143/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0366 - val_loss: 1.9093\n",
      "Epoch 144/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0357 - val_loss: 1.9221\n",
      "Epoch 145/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0351 - val_loss: 1.9069\n",
      "Epoch 146/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0354 - val_loss: 1.9257\n",
      "Epoch 147/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0356 - val_loss: 1.9087\n",
      "Epoch 148/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0348 - val_loss: 1.9139\n",
      "Epoch 149/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0354 - val_loss: 1.9042\n",
      "Epoch 150/150\n",
      "2800/2800 [==============================] - 8s 3ms/step - loss: 0.0362 - val_loss: 1.9221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_1/while/Exit_2:0' shape=(?, 384) dtype=float32>, <tf.Tensor 'lstm_1/while/Exit_3:0' shape=(?, 384) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 32  # Batch size for training.\n",
    "epochs = 150  # Number of epochs to train for.\n",
    "latent_dim = 384  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'Melody_Accompany.csv'\n",
    "lines = pd.read_table(data_path, sep=',' , header=0, names=['melody', 'accompany'], index_col=0)\n",
    "    \n",
    "print(lines.head())\n",
    "\n",
    "melody_source = lines['melody'].values.tolist()\n",
    "acco_source = lines['accompany'].values.tolist()\n",
    "\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for f in melody_source:\n",
    "    f = str(f).strip('[]')\n",
    "    if f == '':\n",
    "        f = 'nan'\n",
    "    txt = [eval(\"'rest'\") if t == 'nan' else eval(t) for t in f.split(',')]\n",
    "    input_seq.append(txt)\n",
    "\n",
    "for f in acco_source:\n",
    "    f = str(f).strip('[]')\n",
    "    if f == '':\n",
    "        f = 'nan'\n",
    "    txt = [eval(\"'rest'\") if t == 'nan' else eval(t) for t in f.split(',')]\n",
    "    txt.insert(0,'\\t')\n",
    "    txt.append('\\n')\n",
    "    target_seq.append(txt)\n",
    "        \n",
    "input_texts = input_seq\n",
    "target_texts =  target_seq\n",
    "\n",
    "input_words = set()\n",
    "target_words = set()\n",
    "\n",
    "for line in input_texts[: min(num_samples, len(lines) - 1)]:\n",
    "    for word in line:\n",
    "        if word not in input_words:\n",
    "            input_words.add(word)\n",
    "for line in target_texts[: min(num_samples, len(lines) - 1)]:\n",
    "    for word in line:\n",
    "        if word not in target_words:\n",
    "            target_words.add(word)\n",
    "\n",
    "            \n",
    "input_characters = sorted(list(input_words))\n",
    "target_characters = sorted(list(target_words))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)\n",
    "# Save model\n",
    "model.save('MA.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "GEIdF72Yxsjv",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Using the trained seq2seq model to predict the accompany for the melody(in list form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "id": "kN53yINixFNu",
    "new_sheet": false,
    "outputId": "b5747a0a-10b5-40ae-ba23-f2aa1d42c645",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   start  length\n",
      "0      0      63\n",
      "1     63      48\n",
      "2    111      79\n",
      "3    190      86\n",
      "4    276     136\n",
      "    0   1   2   3   4   5     6     7\n",
      "0  C4  G4  C5  B3  G4  B4  None  None\n",
      "1  A3  E4  A4  E3  B3  G4  None  None\n",
      "2  A3  E4  A4  E3  B3  G4  None  None\n",
      "3  D3  A3  D4  E4  F4  E4    C4    A3\n",
      "4  C4  G4  E4  C4  G4  E4  None  None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "\n",
    "data_path = 'bar_index.csv'\n",
    "lines = pd.read_table(data_path, sep=',', index_col=0)\n",
    "\n",
    "print(lines.head())\n",
    "bar_index = lines.values.tolist()\n",
    "\n",
    "model = load_model('MA.h5')\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        if (sampled_char != '\\n'):\n",
    "            decoded_sentence.append(sampled_char)\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n",
    "decoded_sentences = []\n",
    "for seq_index in range(bar_index[37][0],bar_index[37][0]+bar_index[37][1]):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    decoded_sentences.append(decoded_sentence)\n",
    "    \n",
    "df_DS = pd.DataFrame(decoded_sentences)\n",
    "print(df_DS.head())\n",
    "df_DS.to_csv('Accompany.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "colab_type": "text",
    "deletable": true,
    "id": "4Zq1gV-TycRd",
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Generate midi files with melody and the accompaniment(in midi stream form) being generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "id": "r72lPLnVYjwQ",
    "new_sheet": false,
    "outputId": "444d3e35-c34c-4500-b1ee-6cde37dccf1c",
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measures count= 33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'test_output.mid'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from music21 import midi, interval, pitch, note, chord, stream\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from collections import Counter\n",
    "\n",
    "data_path = 'Accompany.csv'\n",
    "lines = pd.read_table(data_path, sep=',', index_col=0)\n",
    "list_acco = lines.values.tolist()\n",
    "accompany_list = []\n",
    "for f in list_acco:\n",
    "    txt = [t for t in f if t == t]\n",
    "    txt = [t if (type(t) is str) else '%s'%t for t in txt]\n",
    "    accompany_list.append(txt)\n",
    "accompany_list = [t for t in accompany_list if t != []]\n",
    "\n",
    "data_path_beats = 'beats_of_Accompany.csv'\n",
    "lines_beats = pd.read_table(data_path_beats, sep=',', index_col=0)\n",
    "list_beats = lines_beats.values.tolist()\n",
    "beat_list = []\n",
    "for f in list_beats:\n",
    "    txt = [t for t in f if t == t]\n",
    "    txt = [t if (type(t) is str) else '%s'%t for t in txt]\n",
    "    beat_list.append(txt)\n",
    "beat_list = [t for t in beat_list if t != []]\n",
    "beat_list = sorted(beat_list, key= lambda i:len(i))\n",
    "max_beat_seq_length = max([len(t) for t in beat_list])\n",
    "\n",
    "for i in range(1, max_beat_seq_length + 1):\n",
    "    l = [t for t in beat_list if len(t) == i]\n",
    "    l = [\" \".join(b) for b in l]\n",
    "    if len(l) > 70:\n",
    "        for pattern, count in Counter(l).most_common():\n",
    "            if count < 15:\n",
    "                l = [x for x in l if x != pattern]\n",
    "        locals()['beats_with_%s_note' % i] = l\n",
    "    else:\n",
    "        locals()['beats_with_%s_note' % i] = l\n",
    "    pass\n",
    "\n",
    "midifile_path = \"/gdrive/My Drive/MIDI files/popM/陈奕迅-从何说起.mid\"\n",
    "\n",
    "def setKeys(midi):\n",
    "    keyTonic = midi.analyze('key').tonic\n",
    "    mode = midi.analyze('key').mode\n",
    "    #The pitch of the tone of the original key\n",
    "    originPitch = pitch.Pitch(keyTonic)\n",
    "    #set target key\n",
    "    if mode == \"major\":\n",
    "        targetPitch = pitch.Pitch('C')\n",
    "        pitchInterval = interval.notesToChromatic(originPitch,targetPitch).semitones\n",
    "        if pitchInterval > 6:\n",
    "            pitchInterval = pitchInterval-12\n",
    "        elif pitchInterval < -6:\n",
    "            pitchInterval = pitchInterval+12\n",
    "        if keyTonic != \"C\":\n",
    "            midi.transpose(pitchInterval ,inPlace=True)\n",
    "    elif mode == \"minor\":\n",
    "        targetPitch = pitch.Pitch('A')\n",
    "        pitchInterval = interval.notesToChromatic(originPitch,targetPitch).semitones\n",
    "        if pitchInterval > 6:\n",
    "            pitchInterval = pitchInterval-12\n",
    "        elif pitchInterval < -6:\n",
    "            pitchInterval = pitchInterval+12\n",
    "        if keyTonic != \"A\":\n",
    "            midi.transpose(pitchInterval ,inPlace=True)\n",
    "    return midi\n",
    "\n",
    "midifile = midi.translate.midiFilePathToStream(midifile_path)\n",
    "midifile = setKeys(midifile)\n",
    "\n",
    "melody = midifile.parts[0]\n",
    "melody = melody.makeMeasures()\n",
    "\n",
    "accompaniment = midifile.parts[1]\n",
    "accompaniment = accompaniment.makeMeasures()\n",
    "\n",
    "score = stream.Score()\n",
    "part1 = stream.Part()\n",
    "part1.insert(0,melody)\n",
    "part2 = stream.Part()\n",
    "\n",
    "measure_list = []\n",
    "measures = accompaniment.recurse(classFilter=('Measure'), restoreActiveSites=True).elements\n",
    "for measure in measures:\n",
    "    measure.removeByNotOfClass('Measure')\n",
    "    measure_list.append(measure)\n",
    "measure_stream = stream.Stream(measure_list)\n",
    "\n",
    "accompany_in_measures = []\n",
    "print('measures count=',len(accompany_list))\n",
    "for n in range(len(accompany_list)):\n",
    "    length = len(accompany_list[n])\n",
    "    beat_map = choice(locals()['beats_with_%s_note' % length])\n",
    "    beat_map = [float(b) for b in beat_map.split(' ')]\n",
    "    bar = []\n",
    "    for i in range(len(accompany_list[n])):\n",
    "        ele = accompany_list[n][i]\n",
    "        offset = beat_map[i]\n",
    "        # pattern is a rest\n",
    "        if ele == 'rest':\n",
    "            new_rest = note.Rest()\n",
    "            new_rest.offset = offset\n",
    "            bar.append(new_rest)\n",
    "        # pattern is a chord\n",
    "        elif ('.' in ele) or ele.isdigit():\n",
    "            notes_in_chord = ele.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            bar.append(new_chord)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(ele)\n",
    "            new_note.offset = offset\n",
    "            bar.append(new_note)\n",
    "    accompany_in_measures.append(bar)\n",
    "\n",
    "for i in range(len(measure_stream)):\n",
    "    measure = measure_stream[i]\n",
    "    midi_stream = stream.Stream(accompany_in_measures[i])\n",
    "    measure.insert(0,midi_stream)\n",
    "\n",
    "part2.insert(0,measure_stream)\n",
    "score.insert(0,part1)\n",
    "score.insert(0,part2)\n",
    "\n",
    "score.write('midi',fp = 'test_output.mid')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Automate_accompaniment_with_seq2seq.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
